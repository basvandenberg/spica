#!/usr/bin/env python

import os
import sys
import operator
import argparse
import time
import warnings
import traceback

import numpy

# HACK TODO remove if sklearn is updated to 0.14 on compute servers...
import sklearn
if not(sklearn.__version__ == '0.14.1'):
    sys.path.insert(1, os.environ['SKL'])
    reload(sklearn)
assert(sklearn.__version__ == '0.14.1')

from sklearn.externals import joblib

from spice import classification
from spice import featmat
from biopy import file_io


if __name__ == '__main__':

    # parse arguments
    parser = argparse.ArgumentParser()

    parser.add_argument('-f', '--feature_matrix_dir', required=True)
    parser.add_argument('-l', '--labeling', required=True)
    parser.add_argument('-c', '--classifier', nargs='+', required=True)
    parser.add_argument('-n', '--n_fold_cv', type=int, required=True)
    parser.add_argument('-s', '--feature_selection', required=True)
    parser.add_argument('-e', '--evaluation_score', required=True)

    # root output directory
    parser.add_argument('-o', '--output_dir', required=True)

    parser.add_argument('--standardize', action='store_true', default=False)
    parser.add_argument('--classes', nargs='+', default=None)
    parser.add_argument('--features', nargs='+', default=None)
    parser.add_argument('--feature_file')
    parser.add_argument('--cross_validation_file')

    #parser.add_argument('--lda_weights', action='store_true', default=False)

    # parameter optimization?
    parser.add_argument('--timeout', type=int)  # seconds

    # parameter choices, disregarded if timeout is set
    parser.add_argument('--radius', nargs='+', default=None)
    parser.add_argument('--neighbors', nargs='+', default=None)
    parser.add_argument('--c_parameter', nargs='+', default=None)
    parser.add_argument('--gamma', nargs='+', default=None)

    parser.add_argument('--cpu', type=int, default=1)

    args = parser.parse_args()

    ###########################################################################
    # STEP 1: read feature file and cross-validation file
    ###########################################################################

    # read feature file, if provided
    if(args.feature_file):
        feature_experiments = classification.parse_feature_file(
            args.feature_file)
    else:
        feature_experiments = [('exp', args.features)]

    # read cross-validation file, if provided
    if(args.cross_validation_file):
        cv = file_io.read_cross_validation(args.cross_validation_file)
    else:
        cv = None

    ###########################################################################
    # STEP 2: obtain performance score to use
    ###########################################################################

    # determine what performance score function to use
    if(args.evaluation_score not in classification.all_score_names):
        print('\nIncorrect evaluation score: %s' % (args.evaluation_score))
        print('\nOptions: %s\n' % (', '.join(classification.all_score_names)))
        sys.exit()
    scoring = args.evaluation_score

    ###########################################################################
    # STEP 3: obtain user provided classification parameters
    ###########################################################################

    # store user define parameters
    user_params = {}
    if(args.c_parameter):
        user_params['C'] = [float(p) for p in args.c_parameter]
    if(args.gamma):
        user_params['gamma'] = [float(p) for p in args.gamma]
    if(args.neighbors):
        user_params['n_neighbors'] = [int(p) for p in args.neighbors]
    if(args.radius):
        user_params['radius'] = [float(p) for p in args.radius]

    ###########################################################################
    # STEP 4: load the feature matrix
    ###########################################################################

    # load feature matrix
    print '\nLoading feature matrix...'
    fm = featmat.FeatureMatrix.load_from_dir(args.feature_matrix_dir)
    print 'Done.\n'

    ###########################################################################
    # STEP 5: check number of classes and compatibility with performance score
    ###########################################################################

    # determine how many classes there are
    if(args.classes):
        num_classes = len(args.classes)
    else:
        num_classes = len(fm.labeling_dict[args.labeling].class_names)

    # check if more than one class is provided
    if(num_classes < 2):
        print('\nProvide two or more classes.\n')
        sys.exit()

    # check if evaluation score is possible for given number of classes
    elif(num_classes > 2 and args.evaluation_score == 'roc_auc'):
        print('\nroc_auc only implemented for two class problems.\n')
        sys.exit()

    ###########################################################################
    # STEP 6: create output directory
    ###########################################################################

    # create result dir
    if not(os.path.exists(args.output_dir)):
        os.mkdir(args.output_dir)

    # track runtime
    overall_start_time = int(time.time())

    ###########################################################################
    # LOOP outer: iterate over desired classifiers
    ###########################################################################

    # for each provided classifier
    for classifier_str in args.classifier:

        # create classifier output dir
        cl_d = os.path.join(args.output_dir, classifier_str)
        if not(os.path.exists(cl_d)):
            os.mkdir(cl_d)

        #######################################################################
        # LOOP inner: iterate over desired feature sets
        #######################################################################

        # for each feature set experiment
        for exp_name, feature_list in feature_experiments:

            # create output dir for this experiment
            exp_d = os.path.join(cl_d, exp_name)
            if not(os.path.exists(exp_d)):
                os.mkdir(exp_d)

            ###################################################################
            # Fetch classifier object and scikit-learn data set
            ###################################################################

            # obtain classifier with default parameters set
            cl = classification.get_classifier(classifier_str)

            # obtain scikit-learn dataset
            # NOTE: feature matrix is not standardized)
            # NOTE: if feature_list is None, all features are used
            # NOTE: if args.classes is None, all classes are used
            ds = fm.get_sklearn_dataset(feat_ids=feature_list,
                                        labeling_name=args.labeling,
                                        class_ids=args.classes,
                                        standardized=False)

            # obtain data and target from it
            data = ds.data
            target = ds.target

            ###################################################################
            # Determine the classifier parameter(s/ ranges)
            ###################################################################

            # parameters dictionary
            param = {}

            # iterate over all possible classifier parameters
            for par in classification.classifier_params:

                # check if the current classifier uses this parameter
                if(classifier_str in
                   classification.classifiers_per_param[par]):

                    # use user defined one, if provided
                    if(par in user_params.keys()):
                        param[par] = user_params[par]

                    # use default range otherwise
                    else:
                        param[par] = classification.default_param_range[par]

                        # reduce C param range for rbf kernel
                        if(par == 'C' and classifier_str == 'svc_rbf'):
                            param[par] = 10.0 ** numpy.arange(-1, 2)

                        ''' remove timeout for the moment
                        # adjust range if timeout is provided
                        if(args.timeout and par in timed_param):
                            param[par], run_time = get_timed_parameter_range(
                                cl, data, target, args.standardize,
                                args.timeout, par)

                            # check parameter range
                            if(len(param[par]) == 0):
                                print('Time out occured.\n')
                                sys.exit()
                            elif(len(param[par]) == 1):
                                tmp_param = {par: param[par][0]}
                                cl.set_params(**tmp_param)
                                del param[par]
                            else:
                                pass
                        '''

                    # set parameter
                    if(len(param[par]) == 0):  # only in case of timeout???
                        print 'No value for parameter: %s' % (par)
                        sys.exit(1)
                    elif(len(param[par]) == 1):
                        # set parameter, no grid search required
                        tmp_param = {par: param[par][0]}
                        cl.set_params(**tmp_param)
                        del param[par]
                    else:
                        # otherwise keep in param dict, used to run grid search
                        pass

            ''' remove time estimate for now
            print cl.get_params()
            if(param and args.timeout):
                print param
                # estimate time
                time_estimate = run_time * args.n_fold_cv
                if(classifier_str in classifiers_per_param['gamma']):
                    if 'gamma' in param:
                        time_estimate *= len(param['gamma'])
                print('Estimated run time (sec): %i' % (time_estimate))
            '''

            ###################################################################
            # define output files
            ###################################################################

            settings_f = os.path.join(exp_d, 'settings.txt')
            result_f = os.path.join(exp_d, 'result.txt')
            cm_f = os.path.join(exp_d, 'confusion_matrix.txt')
            gs_f = os.path.join(exp_d, 'grid_search.txt')
            fs_f = os.path.join(exp_d, 'feature_selection.txt')
            param_f = os.path.join(exp_d, 'parameters.txt')
            roc_f = os.path.join(exp_d, 'roc.txt')
            roc_fig_f = os.path.join(exp_d, 'roc.png')
            predictions_f = os.path.join(exp_d, 'predictions.txt')
            all_data_cl_f = os.path.join(exp_d, 'classifier.joblib.pkl')

            ###################################################################
            # RUN EXPERIMENT
            # - cv_score (feature selection 'none')
            # - ffs
            # - bfs
            ###################################################################

            print args.feature_selection

            # catch warnings from lda and qda as exepctions
            warnings.filterwarnings(action='error',
                                    category=RuntimeWarning)

            gs_log_f = open(gs_f, 'w')
            gs_log_f.write('mean,std,cv_scores,parameters\n\n')

            cv_roc_curves = None

            try:

                # run CV experiment without feature selection
                if(args.feature_selection == 'none'):
                    print 'start cv score...'
                    (cv_scores, cv_params, cv_confusion, cv_all_scores,
                        cv_roc_curves, predictions, all_data_cl) =\
                        classification.cv_score(
                            data, target, cl, args.n_fold_cv, scoring,
                            param=param, cv=cv, log_f=gs_log_f, cpu=args.cpu,
                            standardize=args.standardize)
                    cv_feat_is = None

                # run CV experiment with forward feature selection
                # TODO all_data_cl
                elif(args.feature_selection == 'ffs'):
                    print 'start ffs...'
                    (cv_scores, cv_params, cv_confusion, cv_all_scores,
                        cv_roc_curves, cv_feat_is, predictions) =\
                        classification.ffs(
                            data, target, cl, args.n_fold_cv, scoring,
                            param=param, cv=cv, log_f=gs_log_f, cpu=args.cpu,
                            standardize=args.standardize)

                # run CV experiment with backward feature selection
                # TODO all_data_cl
                elif(args.feature_selection == 'bfs'):
                    print 'start bfs...'
                    (cv_scores, cv_params, cv_confusion, cv_all_scores,
                        cv_roc_curves, cv_feat_is, predictions) =\
                        classification.bfs(
                            data, target, cl, args.n_fold_cv, scoring,
                            param=param, cv=cv, log_f=gs_log_f, cpu=args.cpu,
                            standardize=args.standardize)

                else:
                    cv_scores = 'Feature selection method does not exist.'

            except(RuntimeWarning) as e:
                #cv_scores = 'RuntimeWarning occured: %s' % rw
                print traceback.format_exc()
                raise e
                sys.exit()
            except Exception as e:
                print traceback.format_exc()
                raise e
                sys.exit()
            finally:
                gs_log_f.close()

            ###################################################################
            # Write experiment results
            ###################################################################

            # write results to output files
            if(type(cv_scores) == str):
                # a bit of a hack to write error to file
                with open(result_f, 'w') as fout:
                    fout.write(cv_scores)
            else:

                # write settings to file, needs to be improved
                # TODO turn into function
                with open(settings_f, 'w') as fout:
                    fout.write('sample_names,feature_names,target_names,' +
                               'classifier_name,classifier_params,' +
                               'grid_params,n_fold_cv,feature_selection\n')
                    first_sample_names = ds['sample_names'][:10]
                    first_sample_names.append('...')
                    fout.write('%s\n' % (str(first_sample_names)))
                    fout.write('%s\n' % (str(ds['feature_names'])))
                    fout.write('%s\n' % (str(ds['target_names'])))
                    fout.write('%s\n' % (str(classifier_str)))
                    fout.write('%s\n' % (str(cl.get_params())))
                    fout.write('%s\n' % (str(param).replace('\n', '')))
                    fout.write('%i\n' % (args.n_fold_cv))
                    fout.write('%s\n' % (args.feature_selection))

                # write the cv performance results
                with open(result_f, 'w') as fout:
                    fout.write('%s\n' %
                               (','.join(classification.all_score_names)))
                    for index in range(len(classification.all_score_names)):
                        s = [item[index] for item in cv_all_scores]
                        fout.write('%s\n' % (str(s)))

                # write confusion matrices
                with open(cm_f, 'w') as fout:
                    for index, cm in enumerate(cv_confusion):
                        fout.write('CV%i\n' % (index))
                        fout.write('%s\n\n' % (str(cm)))

                # write parameters
                with open(param_f, 'w') as fout:
                    for cv_param in cv_params:
                        fout.write('%s\n' % (str(cv_param)))

                # plot roc curves
                if not(cv_roc_curves.is_empty()):
                    cv_roc_curves.save_avg_roc_plot(roc_fig_f)

                # store classifier trained on full data set
                if not(all_data_cl is None):
                    _ = joblib.dump(all_data_cl, all_data_cl_f, compress=9)

                # sort predictions by object index
                sorted_predictions = sorted(predictions,
                                            key=operator.itemgetter(0))
                preds = zip(fm.object_ids, [p[1] for p in sorted_predictions],
                            [p[2] for p in sorted_predictions])
                file_io.write_tuple_list(predictions_f, preds)

                # write feature selection
                if(cv_feat_is):

                    with open(fs_f, 'w') as fout:
                        fout.write('cv_loop,selected features\n')
                        for index, fs in enumerate(cv_feat_is):
                            fout.write('%i,%s\n' % (index,
                                       '\t'.join([feature_list[fi]
                                       for fi in fs])))

    print('\nRUNTIME: %i' % (int(time.time() - overall_start_time)))
